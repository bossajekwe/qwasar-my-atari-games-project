{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28120,"status":"ok","timestamp":1690184710612,"user":{"displayName":"Moses Ajekwe (Bossajekwe)","userId":"12887297271567106961"},"user_tz":-60},"id":"XmSGG4orUTX7","outputId":"e961584f-05b9-4aad-ac06-2194723daa60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.7.1)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.0\n","Collecting stable-baselines3[extra]\n","  Downloading stable_baselines3-2.0.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gymnasium==0.28.1 (from stable-baselines3[extra])\n","  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.22.4)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.1+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.7.0.72)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.12.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.65.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.4.2)\n","Collecting shimmy[atari]~=0.2.1 (from stable-baselines3[extra])\n","  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (8.4.0)\n","Collecting autorom[accept-rom-license]~=0.6.0 (from stable-baselines3[extra])\n","  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.0)\n","Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1->stable-baselines3[extra])\n","  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]) (4.7.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]) (0.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (8.1.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.27.1)\n","Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra])\n","  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting ale-py~=0.8.1 (from shimmy[atari]~=0.2.1->stable-baselines3[extra])\n","  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.56.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.3)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3[extra]) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3[extra]) (16.0.6)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.41.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2022.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.14.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=0.2.1->stable-baselines3[extra]) (6.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=24ac175704e34aa3612c68797422c2fa28bbd01f4f7f8fcf521f7336f1ef237f\n","  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: jax-jumpy, ale-py, gymnasium, AutoROM.accept-rom-license, autorom, shimmy, stable-baselines3\n","  Attempting uninstall: gymnasium\n","    Found existing installation: gymnasium 0.29.0\n","    Uninstalling gymnasium-0.29.0:\n","      Successfully uninstalled gymnasium-0.29.0\n","Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 gymnasium-0.28.1 jax-jumpy-1.0.0 shimmy-0.2.1 stable-baselines3-2.0.0\n"]}],"source":["!pip install gymnasium\n","!pip install stable-baselines3[extra]"]},{"cell_type":"markdown","metadata":{"id":"_axp7tInITj_"},"source":["# importing the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bu3nH1OnDm88"},"outputs":[],"source":["import os\n","import gymnasium as gym\n","import numpy as np\n","from stable_baselines3 import DQN\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.callbacks import EvalCallback,StopTrainingOnRewardThreshold\n","from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n","from stable_baselines3.dqn.policies import CnnPolicy\n","from gymnasium.utils.save_video import save_video\n","from gymnasium.wrappers import FrameStack,  ResizeObservation\n","from PIL import Image\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"markdown","metadata":{"id":"Klfgf7Hu9DNw"},"source":["# CartPole Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfSErqIyqlVU"},"outputs":[],"source":["class CartPoleDQNAgent:\n","    def __init__(self, name=None, env_name=None,eval_freq=20000, buffer_size=1000):\n","        self.name = name # name of the game\n","        self.env_name = env_name # environment name\n","        self.policy = \"MlpPolicy\" # policy\n","        self.eval_freq = eval_freq # evaluation frequency\n","        self.buffer_size = buffer_size # buffer size for the replay buffer\n","        self.log_path = os.path.join('/content/drive/MyDrive/Colab_Notebooks/my_atari_games/Training/DQN_' + self.name + '_Log') # path for loging the training  data\n","        self.save_path = os.path.join('/content/drive/MyDrive/Colab_Notebooks/my_atari_games/Saved_Models/DQN_' + self.name +'_Model') # path for saving the trained model\n","        self.env = self.make_environment() #function that creates the environment and agent.\n","        self.model = self._build_dqn() #function that builds the DQN model.\n","\n","    def make_environment(self): # A call to the function that creates the environment\n","        env = gym.make(self.env_name, render_mode=\"rgb_array\") # creates the environment and agent\n","        env = DummyVecEnv([lambda: env]) #creates a vectorized dummy environment\n","        return env # returns the created  environment.\n","\n","    def _build_dqn(self): # A call to the function that builds the DQN model\n","        model = DQN(self.policy, self.env, verbose=0, tensorboard_log=self.log_path, buffer_size=self.buffer_size) # creates the DQN model\n","        return model # returns the created DQN model\n","\n","\n","    def _play_one_episode(self): # A call to the function that plays one episode\n","          obs = self.env.reset() # resets the environment\n","          done = False # sets the done flag\n","          score = 0 # sets the score to zero\n","\n","          while not done: # loops until the done flag is set\n","              action = self.env.action_space.sample() # selects an action from a sample space\n","              obs, reward, done, *info= self.env.step([action]) # takes the action and returns the observation, reward, done, and info\n","              score += reward # Updates the score\n","\n","          return score # returns the score value\n","\n","\n","    def play_episodes(self, num_episodes=10, play_type =\"random\"): # A call to the function that plays episodes\n","        if play_type == \"random\": # if the play type is random\n","          print(f\"Playing the {self.name} game randomly for {num_episodes} episodes\") # prints the message\n","          scores = [self._play_one_episode() for _ in range(num_episodes)] # creates a list of scores\n","          for episode, score in enumerate(scores, 1): # loops through the list of scores\n","            print(f\"Episode {episode}: {score[0]}\") # prints the score\n","\n","        if play_type == \"predict\": # if the play type is predict\n","          episode_rewards = [] # creates a list of episode rewards\n","          frames = [] # creates a list of frames for the images\n","\n","          for episode in range(num_episodes): # loops through the number of episodes\n","              obs = self.env.reset() # resets the environment\n","              done = False # sets the done flag\n","              score = 0 # sets the score to zero\n","\n","              while not done: # loops until the done flag is set\n","                  action, _ = self.model.predict(obs) # predicts the action to take from the observation\n","                  obs, reward, done, *info= self.env.step(action) # takes the action and returns the observation, reward, done, and info\n","                  score += reward # Updates the score\n","                  frame = Image.fromarray(self.env.render()) #Craptures the frame of image  from the environment\n","                  frame = np.array(frame) # converts the frame to numpy\n","                  frames.append(frame) # adds the frame to the list\n","\n","              episode_rewards.append(score) # adds the score to the list\n","\n","              print(f\"Episode {episode+1}: {score[0]}\") # prints the score\n","\n","          video_path =  os.path.join(self.save_path, self.name + \"_Agent_play\") # video path\n","\n","\n","          save_video(frames, video_path, fps= 30, name_prefix =f\"{self.name}-agent-play\") # saves the video\n","\n","\n","    def train(self, time_steps=None, stop_value=None): # A call to the function that trains the agent\n","        stop_callback = StopTrainingOnRewardThreshold(reward_threshold=stop_value, verbose=0) # creates the stop callback, assigns the reward threshold so training can stop\n","        eval_callback = EvalCallback(self.env, callback_on_new_best=stop_callback,\n","                                     eval_freq=self.eval_freq, best_model_save_path=self.save_path) # creates the eval callback, checks if the reward has been achieved\n","        self.model.learn(total_timesteps=time_steps, callback=eval_callback) # trains the model\n","\n","\n","    def evaluate_policy(self, episodes=None): # A call to the function that evaluates the policy\n","        mean_reward, reward_std = evaluate_policy(self.model, self.env, n_eval_episodes=episodes) # evaluates the policy\n","        print(f\"Mean reward over {episodes} episodes is {mean_reward} with a standard deviation of {reward_std}\") # prints the mean reward and standard deviation\n","\n","    def close_env(self): # A call to the function that closes the environment\n","        self.env.close() # closes the environment\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2SNSMa3qlYH"},"outputs":[],"source":["#create the agent and create the environment\n","CartPole_agent = CartPoleDQNAgent(name=\"CartPole\", env_name=\"CartPole-v1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEjjHwaZAQqZ"},"outputs":[],"source":["#Play the cart pole game randomly for 20 episodes\n","CartPole_agent.play_episodes(num_episodes=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgBhDR1BAQuF"},"outputs":[],"source":["#test out the agent with the cart pole game\n","CartPole_agent.train(time_steps=1000000, stop_value=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yd9sDTVGAQxp"},"outputs":[],"source":["#test out the agent with the cart pole game\n","CartPole_agent.play_episodes(num_episodes=10, play_type=\"predict\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQOB73FM1Yru"},"outputs":[],"source":["#close the environment\n","CartPole_agent.close_env()"]},{"cell_type":"markdown","metadata":{"id":"Yp4V6nG4GEGb"},"source":["# DQNAgent for SpaceInvaders and Pac-Man"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":359,"status":"ok","timestamp":1690184871013,"user":{"displayName":"Moses Ajekwe (Bossajekwe)","userId":"12887297271567106961"},"user_tz":-60},"id":"AxHI8VtwAQ0Q"},"outputs":[],"source":["\n","class DQNAgent: # A class that creates the DQN model\n","    def __init__(self, name=None, env_name=None,eval_freq=20000, buffer_size=1000):\n","        self.name = name # name of the game\n","        self.env_name = env_name# environment name\n","        # self.policy = \"MultiInputPolicy\" # policy\n","        self.eval_freq = eval_freq # evaluation frequency\n","        self.buffer_size = buffer_size # buffer size for the replay buffer\n","        self.log_path = os.path.join('/content/drive/MyDrive/Colab_Notebooks/my_atari_games/Training/DQN_' + self.name + '_Log') # path for loging the training  data\n","        self.save_path = os.path.join('/content/drive/MyDrive/Colab_Notebooks/my_atari_games/Saved_Models/DQN_' + self.name +'_Model') # path for saving the trained model\n","        self.env = self.make_environment() #function that creates the environment\n","        self.model = self._build_dqn() #function that builds the DQN model\n","\n","    def make_environment(self): # A call to the function that creates the environment\n","        env = gym.make(self.env_name, render_mode=\"rgb_array\") # creates the environment and agent\n","        env =  ResizeObservation(env,84) #Resize the observation\n","        # env = FrameStack(env, num_stack=4) # stacks the frames\n","        return env # returns the created  environment.\n","\n","    def _build_dqn(self): # A call to the function that builds the DQN model\n","        model = DQN(CnnPolicy, self.env, verbose=0, tensorboard_log=self.log_path, buffer_size=self.buffer_size) # creates the DQN model\n","        return model # returns the created DQN model\n","\n","    def _play_one_episode(self): # A call to the function that plays one episode\n","        obs, _ = self.env.reset() # resets the environment\n","        done = False # sets the done flag\n","        score = 0 # sets the score to zero\n","\n","        while not done: # loops until the done flag is set\n","            action= self.env.action_space.sample()  # selects an action from a sample space randomly\n","            obs, reward, done, *info = self.env.step(action) # takes the action and returns the observation, reward, done, and info\n","            score += reward # Updates the score\n","\n","        return score # returns the score value\n","\n","\n","    def play_episodes(self, num_episodes=10, play_type =\"random\"): # A call to the function that plays episodes\n","        if play_type == \"random\": # if the play type is random\n","          print(f\"Playing the {self.name} game randomly for {num_episodes} episodes\") # prints the message\n","          scores = [self._play_one_episode() for _ in range(num_episodes)] # creates a list of scores\n","          for episode, score in enumerate(scores, 1): # loops through the list of scores\n","            print(f\"Episode {episode}: {score}\") # prints the score\n","\n","        if play_type == \"predict\": # if the play type is predict\n","          episode_rewards = [] # creates a list of episode rewards\n","          frames = [] # creates a list of frames for the images\n","\n","          for episode in range(num_episodes): # loops through the number of episodes\n","              obs, _ = self.env.reset() # resets the environment\n","              done = False # sets the done flag\n","              score = 0 # sets the score to zero\n","\n","              while not done: # loops until the done flag is set\n","                  action, _ = self.model.predict(obs) # predicts the action to take from the observation\n","                  obs, reward, done, *info= self.env.step(action) # takes the action and returns the observation, reward, done, and info\n","                  score += reward # Updates the score\n","                  frame = Image.fromarray(self.env.render()) #Craptures the frame of image  from the environment\n","                  frame = np.array(frame) # converts the frame to numpy\n","                  frames.append(frame)# adds the frame to the list\n","\n","              episode_rewards.append(score) # adds the score to the list\n","\n","              print(f\"Episode {episode+1}: {score}\")# prints the score\n","\n","          video_path =  os.path.join(self.save_path, self.name + \"_Agent_play\") # video path\n","\n","\n","          save_video(frames, video_path, fps=30, name_prefix =f\"{self.name}-agent-play\") # saves the video\n","\n","\n","    def train(self, time_steps=None, stop_value=None): # A call to the function that trains the agent\n","        stop_callback = StopTrainingOnRewardThreshold(reward_threshold=stop_value, verbose=0) # creates the stop callback, assigns the reward threshold so training can stop\n","        eval_callback = EvalCallback(self.env, callback_on_new_best=stop_callback,\n","                                     eval_freq=self.eval_freq, best_model_save_path=self.save_path) # creates the eval callback, checks if the reward has been achieved\n","        self.model.learn(total_timesteps=time_steps, callback=eval_callback) # trains the model\n","\n","    def evaluate_policy(self, episodes=None): # A call to the function that evaluates the policy\n","        mean_reward, reward_std = evaluate_policy(self.model, self.env, n_eval_episodes=episodes) # evaluates the policy\n","        print(f\"Mean reward over {episodes} episodes is {mean_reward} with a standard deviation of {reward_std}\") # prints the mean reward and standard deviation\n","\n","    def load_best_model(self):\n","        best_model = DQN.load(self.save_path + \"/best_model\")\n","        return best_model\n","\n","    def save_model(self):\n","        return self.model.save(self.save_path)\n","\n","    def close_env(self): # A call to the function that closes the environment\n","        self.env.close() # closes the environment\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lO63ctPA0v5g"},"source":["# SpaceInvaders"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":433,"status":"ok","timestamp":1690185067091,"user":{"displayName":"Moses Ajekwe (Bossajekwe)","userId":"12887297271567106961"},"user_tz":-60},"id":"mrpuXIvoHVZq"},"outputs":[],"source":["#initialize the agent and create the environment\n","SpaceInvaders_agent = DQNAgent(name=\"SpaceInvaders\", env_name=\"SpaceInvaders-v4\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_PgT3VNHVkg"},"outputs":[],"source":["#Play the space invaders game randomly for 20 episodes\n","SpaceInvaders_agent.play_episodes(num_episodes=20)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"x6oZsnOcHVss","executionInfo":{"status":"ok","timestamp":1690189025112,"user_tz":-60,"elapsed":29,"user":{"displayName":"Moses Ajekwe (Bossajekwe)","userId":"12887297271567106961"}}},"outputs":[],"source":["#train the agent\n","SpaceInvaders_agent.train(time_steps=1000000, stop_value=1000)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52035,"status":"ok","timestamp":1690188914599,"user":{"displayName":"Moses Ajekwe (Bossajekwe)","userId":"12887297271567106961"},"user_tz":-60},"id":"wk6znVXONYMq","outputId":"7ec15a9a-8057-4b77-da1e-e99e250d6478"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean reward over 10 episodes is 205.0 with a standard deviation of 90.69178573608527\n"]}],"source":["SpaceInvaders_agent.evaluate_policy(episodes=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nX3848uYH1Wv"},"outputs":[],"source":["# test out the agent with the space invaders game\n","SpaceInvaders_agent.play_episodes(num_episodes=10, play_type=\"predict\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3Gl36T4MTb0"},"outputs":[],"source":["#Close the environment\n","SpaceInvaders_agent.close_env()"]},{"cell_type":"markdown","metadata":{"id":"2TptTs3RzbZp"},"source":["# Pacman"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bi599sNy-Sc"},"outputs":[],"source":["#initialize the agent and create the environment\n","Pacman_agent_agent = DQNAgent(name=\"Pacman\", env_name=\"MsPacman-v4\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xN0bOZrEy-bd"},"outputs":[],"source":["#Play the pacman game randomly for 20 episodes\n","Pacman_agent_agent.play_episodes(num_episodes=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOo_tlGiy-oh"},"outputs":[],"source":["#train the agent\n","Pacman_agent_agent.train(time_steps=1000000, stop_value=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X02-fj5RzPQT"},"outputs":[],"source":["#evaluate the policy used by the agent\n","Pacman_agent_agent.evaluate_policy(episodes=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zEwXh6nO1BYo"},"outputs":[],"source":["Pacman_agent_agent.save_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWssbaeszTMi"},"outputs":[],"source":["# test out the agent with the pacman game\n","Pacman_agent_agent.play_episodes(num_episodes=10, play_type=\"predict\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8kAQe2PzX2T"},"outputs":[],"source":["#Close the environment\n","Pacman_agent_agent.close_env()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tib-T84N6K_q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EYiekUC6LKm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"101WNHEDWaPVt34vHdDwOX2ywvPi41_bC","authorship_tag":"ABX9TyNZAREtY7jO/sW1RcZuwSbP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}